\chapter{Literature Review}
\label{chap:lr}
\chaptermark{Literature Review}

\section{sEMG-Based Hand Pose Estimation}

Surface electromyography (sEMG) is commonly employed to infer hand gestures and joint angles by recording the electrical signals generated in the forearm muscles \cite{oskoei2007myoelectric, simao2019review}. Classic pipelines begin with manually engineered signal descriptors -- like mean absolute value, waveform length, or zero-crossing counts -- and then feed them into classifiers such as Support Vector Machines (SVMs) or Multilayer Perceptrons (MLPs) \cite{oladazimi2012review, liu2007recognition}. Yet these approaches usually struggle to stay reliable when the motions get more complex or the EMG signals vary \cite{parajuli2019real}. On top of that, crafting strong hand-made features usually demands specialized know-how and plenty of expert tweaking, which naturally holds back how well the method transfers to new tasks or users and how easily it can scale \cite{atzori2016deep, oskoei2008support, phinyomark2018feature}.

Recent work has focused on deep learning models, researchers have shifted their attention to deep-learning architectures such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and temporal convolutional networks (TCNs) \cite{ameri2019regression, briouza2021convolutional, zhang2023lstm}. These networks can pull out time-based patterns straight from raw or lightly processed sEMG. Even though they usually beat the classic feature-engineering pipelines in accuracy, they still stick to dynamic pattern search and are expensive at compute. \cite{lee2022explainable}.

To address this gap, some studies have turned to attention-driven and adaptive networks that can zero in on the most informative parts of the EMG signal \cite{yang2025stcnet, hu2019semg}. Yet, few works introduce generally new novel techniques, especially the ones aiming to reduce compute by leveraging the signal nature itself. Our proposed approach, termed \textit{Spatiotemporal Sampling}, aims to fill this gap by learning a sampling window and spatial activation pattern jointly from the data.

\section{Multimodal sEMG Datasets}

Several public datasets now support training and evaluating sEMG-based models. Among them, the \textbf{NinaPro} series is the go-to choice, pairing synchronized sEMG signals with kinematic-glove readings across many gesture types \cite{zia2018multiday}. Despite its usefulness, NinaPro centers on brief, scripted motions and cross-subject generalization, so it is less ideal for single-user studies that need longer, continuous recording sessions.

The \textbf{EMG2Pose} dataset represents one of the most ambitious publicly released resources for fine-grained pose regression to date \cite{salter2024emg2pose}. It blends high-resolution 3-D motion-capture data with simultaneously recorded sEMG signals obtained from 26 OptiTrack cameras and state-of-the-art Delsys electrodes -- an impressive yet cost-prohibitive arrangement for most academic laboratories. In a similar vein, leading studies on continuous hand tracking routinely acquire sEMG at sampling rates near 2 kHz and with at least 12-bit resolution -- technical choices that safeguard subtle muscle-activation details and the sub-millisecond timing needed for accurate joint-angle estimation \cite{salter2024emg2pose, zanghieri2023semg, lee2022explainable}. Consequently, these hardware specifications have become the de facto yardstick for high-resolution EMG-to-pose research, setting a performance bar that lower-cost systems must now strive to meet.

At the opposite end of the price range, a variety of very low-cost rigs have appeared -- often built around consumer devices such as the Myo armband or custom EMG boards powered by Arduino controllers \cite{nasri2020semg}. Although appealing for their affordability, these setups usually face two major hurdles:
\begin{itemize}
    \item Sampling resolution and rates that fall short of the \(\sim\!12\text{-bit}\), \(2\,\text{kHz}\) baseline;
    \item No reliably synchronized, continuous hand-tracking stream at \(30\,\text{fps}\) or higher \cite{graf2023combining}.
\end{itemize}

\section{3D Hand Tracking with MediaPipe}

MediaPipe is the leading open-source toolkit for 3-D hand tracking in both academic studies and real-world products. Released by Google, it bundles a full graph-based vision pipeline, delivering real-time (30-60 fps) landmark detection, temporal filtering, and gesture classification from nothing more than a standard RGB camera. Because the entire framework is cross-platform -- running on desktop CPUs, mobile phones, and even tiny single-board computers -- it lowers the barrier to entry for labs and start-ups that can`t afford specialised depth sensors.

Unlike closed systems such as the Leap Motion controller or most VR headsets -- which lock users into proprietary hardware, fixed SDKs, and a cramped capture volume of roughly 0.5 - 1 m -- MediaPipe imposes no hard tracking boundary. The same model can follow hands at arm`s length, across a lab bench, or within a wide-angle studio shot, enabling what we call \textit{unbounded spatial scaling}. Its permissive licence also lets researchers tweak the network architecture, integrate domain-specific priors (e.g., EMG synchronisation), and port the solution to custom embedded boards -- fostering rapid experimentation that would be impossible on locked-down commercial stacks. In short, MediaPipe blends affordability, flexibility, and high frame-rate performance, making it an ideal backbone for our cost-efficient, multimodal data-collection system.

Several independent investigations now provide quantitative proof that MediaPipe delivers both high accuracy and robustness across a broad spectrum of experimental conditions. For example, Reimer \textit{et al.} \cite{reimer2023evaluation} ran a room-scale benchmark showing that the framework can keep a stable lock on the hand skeleton at viewing distances of up to 2.75 m -- a range in which the Oculus Quest optical tracker and the Leap Motion controller already begin to show noticeable drop-outs, especially when multiple users move simultaneously in the same capture space.  

When the focus shifts to fine-grained finger-joint reconstruction, Maggioni \textit{et al.} \cite{maggioni2025optimisation} recorded a RMSE of just \(10.9^{\circ}\) joint angle error for MediaPipe -- comfortably below the \(14.7^{\circ}\) reported for Leap Motion in the identical task -- underlining the framework`s superior kinematic fidelity.  

Beyond pure motion capture, Lee \textit{et al.} \cite{lee2022explainable} have already integrated MediaPipe-derived landmarks into deep-learning pipelines for sEMG-based gesture recognition, further reinforcing its value as a dependable, high-quality data source for multimodal research and practical applications.

Taken together, these strengths make MediaPipe an ever more compelling solution for conventional motion-capture systems -- particularly in research or industrial settings that need 3-D hand-pose tracking which scales easily, stays budget-friendly, and still meets strict accuracy requirements.

\section{Deep Learning Models for EMG-to-Pose Estimation}

Early deep-learning efforts showed that both convolutional and recurrent networks can handle \textit{sEMG} signals for hand-gesture recognition. Briouza \textit{et al.} \cite{briouza2021convolutional}, for example, built a compact 1-D CNN that reached high accuracy on the Ninapro DB2 benchmark (49 gestures across 40 participants). A newer study by Zhang \textit{et al.} \cite{zhang2023lstm} introduced an LSTM with a two-stage attention module (LSTM-MSA) that surpassed earlier models on several EMG datasets, boosting F1-score, accuracy, recall, and precision. These findings underline how adding temporal context and learned attention can greatly improve sEMG decoding.

The field is now shifting from classifying discrete gestures to \textit{continuous pose estimation}. In this newer line of work, networks aim to map raw sEMG directly to full hand kinematics, predicting joint angles frame by frame.

\textbf{VEMG2Pose} -- introduced together with the EMG2Pose dataset -- adopts a velocity-first decoding strategy \cite{salter2024emg2pose}. A time-depth separable CNN encoder first grabs features, after which an LSTM predicts angular velocities from the sEMG and integrates them over time into joint angles. This autoregressive loop smooths trajectories and keeps drift in check. Even though the network has only a few million parameters -- roughly on par with other baselines -- it still delivers state-of-the-art results. In EMG2Pose`s toughest split (unseen users and unseen motion stages), it posted the lowest mean fingertip landmark error at \textbf{21.6 \textpm 2.0 mm}, outperforming NeuroPose (24.9 \textpm 1.7 mm) and SensingDynamics (27.2 \textpm 2.0 mm) under the same protocol. Its mean joint-angle error in that scenario was 15.8$^\circ$, versus 17.5$^\circ$ and 18.7$^\circ$ for the competing models. VEMG2Pose also leads in the easier generalization tasks -- evidence that velocity decoding boosts cross-user robustness. The model further benefits from being trained on the massive EMG2Pose collection comprising \textbf{193 subjects and 370 hours} of sEMG-motion data, which likely enhances its ability to cope with varied users and gestures.

\textbf{NeuroPose} takes a different route -- it performs \emph{direct} joint-angle regression with a U-Net-style encoder-decoder built from residual bottleneck blocks \cite{liu2021neuropose}. A convolutional encoder first compresses the multi-channel sEMG signal in both space and time, residual layers refine the features, and a decoder upsamples the representation back to the original rate to output joint angles frame by frame.

The earliest version by Liu \textit{et al.} (2021) -- trained on a modest 8-channel Myo armband with only 12 users -- employed anatomical finger-motion constraints plus transfer learning to adapt to new participants. It reported a median angular error of roughly \textbf{6.24$^\circ$} (90th percentile 18.3$^\circ$) and could run in real time on a smartphone with about 0.1 s latency.

To boost interpretability, newer variants weave in attention mechanisms. For instance, Lee \textit{et al.} \cite{lee2022explainable} inserted a channel-wise attention block that highlights which electrode streams matter most for each predicted finger angle, yielding clear saliency maps that align with known muscle activations.

When NeuroPose was retrained from scratch on the larger 16-channel \textit{EMG2Pose} dataset, it remained competitive. Yet, its errors were still higher than those of VEMG2Pose -- for example, a fingertip landmark error of about \textbf{24.9 mm} in the toughest unseen-user split, roughly 15 \% worse \cite{salter2024emg2pose}.

The encoder-decoder (on the order of a few million parameters) and the built-in temporal-smoothing loss do produce smooth trajectories, but the model appears to generalise less effectively to entirely new users without additional adaptation.

\textbf{SensingDynamics} explores yet another design trade-off, pairing \textit{high-density EMG} acquisition with a deliberately lightweight network architecture -- \cite{simpetru10sensing}.
S\^{i}mpetru \textit{et~al.} (2022) recorded signals from a \textit{320-electrode} forearm array distributed across five patches in 13 participants. Their model uses a custom 3-D convolutional encoder to digest the 320-channel spatio-temporal input, followed by a compact three-layer MLP that simultaneously predicts joint angles, key-point positions, and even grip force. According to their report, the system can \emph{"sense the full dynamics"} of the hand, reaching within-subject fingertip errors as low as \textbf{3 mm} (MAE).

Because the network was tuned specifically for this dense layout, it includes layers such as learnable synthetic-muscle-unit (SMU) activations, 3-D convolutions that span multiple electrode patches, and a parallel low-pass EMG branch to boost noise robustness. Even with these extras, SensingDynamics remains the most parameter-efficient of the three baselines -- only a handful of convolutional blocks plus a small MLP. That compactness, however, comes with a trade-off: on the larger cross-user benchmark it lags behind the other models, posting roughly \textbf{27.2 mm} mean fingertip error in the unseen-user-and-stage split -- versus 24.9 mm for NeuroPose and 21.6 mm for VEMG2Pose. In short, its streamlined architecture excels on its original high-density, per-user data but struggles to capture the fuller signal complexity required for broad generalisation.

All three baseline models were re-examined under a single evaluation pipeline in the EMG2Pose study \cite{salter2024emg2pose}, after being retrained on the project`s full corpus of roughly \(\sim\!80\) million frames from 193 participants. The head-to-head results highlight three main takeaways.  
\textbf{First}, networks that treat temporal dynamics explicitly -- as VEMG2Pose does through its velocity-integration loop -- deliver smoother trajectories and lower errors.  
\textbf{Second}, generalisation hinges on both model capacity and data scale: SensingDynamics, although highly accurate in its original per-user, high-density setting, falls behind on this broader, more varied dataset.  
\textbf{Third}, interpretability add-ons -- for example, the channel-wise attention in newer NeuroPose variants -- are becoming essential for diagnosing and refining EMG-to-pose pipelines.  

Informed by these lessons, the present work introduces an \emph{adaptive spatio-temporal sampling} block that automatically highlights the most informative electrodes and time windows on a per-instance basis, aiming to boost intra-user robustness and enhance overall decoding reliability.

\section{Research Gap and Questions}

Interest in sEMG-based hand-pose estimation keeps growing, yet clear shortcomings remain in both model design and data-collection practice. This thesis tackles those shortcomings by proposing a fresh modelling framework and by building an affordable multimodal acquisition pipeline.

\subsection*{Gap 1 -- Missing Joint Spatiotemporal Feature Selection}

Most modern networks still rely on a fixed-length time window and treat every channel the same. That setup offers little control over \emph{where} and \emph{when} to focus inside the signal -- ignoring cues such as electromechanical delay or the fact that some electrodes matter more for certain motions. The result is less interpretability and extra, often wasted, computation.

To close this gap, we introduce \textit{Spatiotemporal Sampling} -- a block that learns, straight from the data, which time slices and electrodes carry the most useful information.

\textbf{RQ1.} How can a model learn to pick the right temporal snippets and spatial patterns in a multichannel sEMG stream?

\textbf{RQ2.} How can knowledge about electromechanical delay and signal alignment guide that sampling process?

\subsection*{Gap 2 -- Few Low-Cost Systems for High-Quality Multimodal Capture}

Existing multimodal datasets usually depend on price-heavy motion-capture rigs or give up temporal resolution and sync accuracy. That barrier makes it hard to gather long, continuous recordings -- the kind real-world apps actually need.

We therefore build a budget-friendly pipeline that combines MediaPipe 3-D tracking with custom EMG hardware, both locked to a shared clock.

\textbf{RQ3.} Which hardware-plus-software choices allow us to record high-rate sEMG and 3-D hand pose in perfect sync, yet keep costs low?

\textbf{RQ4.} How should we run the data-collection sessions so that we capture long, natural hand movements aligned with sEMG?

\subsection*{Gap 3 -- Limited Study of Intra-Subject Generalisation}

Most public corpora focus on cross-user generalisation over short, scripted gestures. Real-life scenarios -- like personalised UIs -- need a model that stays reliable within the \emph{same} user during long, free-form motion.

Our new dataset targets that need, recording lengthy sessions from a single user, and our model is built to adapt to the subtle changes found there.

\textbf{RQ5.} How can we organise a dataset to study intra-subject generalisation under realistic, continuous hand movements?

\textbf{RQ6.} What traits must a model have to exploit that intra-subject variation during training and inference?
