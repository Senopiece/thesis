\chapter{Evaluation and Discussion}
\label{chap:eval}
\chaptermark{Evaluation and Discussion}

\section{Sweep Results}

\subsection{Hyperparameter-Error Correlation Analysis}
Based on our exhaustive hyperparameter sweep, we identified the correlation between hyperparameters and validation performance.

Most of the shared hyperparameters exhibit a \textbf{neutral correlation} with validation performance. These include: 
\texttt{slmerr\_k}, \texttt{batch\_size}, \texttt{train\_prediction\_length}, \texttt{emg\_samples\_per\_frame}, and \texttt{predict\_hidden\_layer\_size}.

However, a few hyperparameters show consistent \textbf{non-neutral correlation} across both architectures:
\begin{itemize}
    \item \texttt{train\_length} and \texttt{l2} exhibit a \textbf{positive correlation} in both CNN and STS setups.
    \item \texttt{poses\_in\_context} shows a \textbf{negative correlation} in both CNN and STS setups.
\end{itemize}

Several hyperparameters exhibit \textbf{disagreement} between the CNN and STS setups, where one model shows a distinct correlation while the other remains largely unaffected. Specifically, 
\texttt{vel\_k} and \texttt{train\_sample\_ratio} show a \textbf{negative correlation} in the CNN setup but fall back to \textbf{neutral} in STS. 
Conversely, \texttt{lr} and \texttt{context\_span} exhibit a \textbf{positive correlation} in STS, while remaining \textbf{neutral} in CNN. 
Similarly, \texttt{accel\_k} and \texttt{synapse\_features} show a \textbf{negative correlation} in STS, but again \textbf{neutral} in CNN. 
This pattern suggests these hyperparameters are more architecture-sensitive and their influence does not generalize across models.

In some cases, the correlation not only differs in strength but in \textit{direction} between architectures. 
Notably, \texttt{train\_patches} exhibits a \textbf{positive correlation} in CNN but a \textbf{negative correlation} in STS. 
Likewise, both \texttt{frames\_per\_window} and \texttt{l1} show a \textbf{negative correlation} in CNN and a \textbf{positive correlation} in STS, further emphasizing divergent optimization behavior.

Interestingly, the hyperparameters allowing train split to grow larger, namely \texttt{train\_length}, \texttt{train\_sample\_ratio} and \texttt{train\_patches}, show a very diverse correlation with the validation metric -- hence a deeper investigation was conducted to evaluate non-linear dependencies. As the result, it was found that the dependency is indeed non-linear neither any monotonic. It appears to be that the optimal range of the interval to train is \textbf{14-18 minutes}, a bigger split harms the prediction results. Such a  investigation imply that the slow-varying drift, subject fatigue, and nonstationary artifacts has a severe affect in a long term sessions, disallowing for a model to keep being effective.

Among the architecture-specific hyperparameters, distinct trends were also observed. 
In the CNN setup, both \texttt{conv\_layer1\_kernels} and \texttt{conv\_layer2\_kernels} exhibited a \textbf{strong positive correlation} with the validation mean landmark error, 
indicating that larger kernel counts may lead to overfitting or excess model capacity. 
In contrast, \texttt{conv\_out\_kernels} demonstrated a \textbf{neutral correlation}, at the same time having low importance metric -- suggesting that model can broadly adopt without overfitting even if having redundant output size.

In the STS setup, all model-specific hyperparameters — namely, 
\texttt{slice\_width}, \texttt{std\_width}, \texttt{mx\_width}, \texttt{std\_stride}, \texttt{slices}, \texttt{patterns}, and \texttt{mx\_stride} — showed consistently \textbf{neutral correlation} 
with model performance. This suggests that while these parameters control the spatial decomposition of input features, they do not critically impact model generalization within the tested range -- indeed the importance metric for all of them is also low, suggesting that either that the default STS can already capture meaningful features good enough or it.

\subsection{Hyperparameter Importance Analysis}
We also identified the parameters whose deviations mostly affect model`s performance.  
The rankings below list these \emph{most sensitive} hyperparameters for each architecture.

\begin{enumerate}
    \item \textbf{STS setup - most sensitive hyperparameters}
    \begin{itemize}
        \item \textbf{muscle\_features}
        \item \textbf{l1}
        \item \textbf{l2}
        \item \textbf{frames\_per\_window}
    \end{itemize}

    \item \textbf{CNN setup - most sensitive hyperparameters}
    \begin{itemize}
        \item \textbf{conv\_layer2\_kernels}
        \item \textbf{l2}
        \item \textbf{train\_sample\_ratio}
        \item \textbf{synapse\_features}
    \end{itemize}
\end{enumerate}

\section{Inter-Subject Exploration and Fit Without EMG}

\paragraph{Selected Hyperparameters}

Following the hyperparameter correlation and importance analysis, we picked the following values for the evaluation on the second stage:

{\small
\begin{longtable}{l|c|c}
\captionsetup{justification=centering}
\caption[Hyperparameter Values for STS and CNN]{Hyperparameter values used in STS and CNN setups.} \label{tab:hypers_values} \\
\hline
\textbf{Hyperparameter} & \textbf{STS Value} & \textbf{CNN Value} \\
\hline
\endfirsthead
\multicolumn{3}{@{}l}{\tablename\ \thetable{} -- continued from previous page} \\
\hline
\textbf{Hyperparameter} & \textbf{STS Value} & \textbf{CNN Value} \\
\hline
\endhead
emg\_samples\_per\_frame       & 32           & 32           \\
slmerr\_k                      & 1.0          & 1.0          \\
vel\_k                         & 1.0          & 1.0          \\
accel\_k                       & 10.0         & 10.0         \\
lr                             & 1e-4         & 1e-4         \\
l1                             & 1e-4         & 1e-1         \\
l2                             & 1e-4         & 1e-4         \\
context\_span                  & 512 ms       & 512 ms       \\
poses\_in\_context             & 6            & 6            \\
conv\_layer1\_kernels          & ---          & 32           \\
conv\_layer2\_kernels          & ---          & 32           \\
conv\_out\_kernels             & ---          & 32           \\
synapse\_features              & 512          & 512          \\
muscle\_features               & 32           & 32           \\
predict\_hidden\_layer\_size   & 128          & 128          \\
batch\_size                    & 32           & 32           \\
train\_length                  & 14 min       & 14 min       \\
train\_prediction\_length      & 2 s          & 2 s          \\
train\_patches                 & 20000        & 20000        \\
train\_sample\_ratio           & 0.05         & 0.05         \\
slices                         & 32           & ---          \\
patterns                       & 32           & ---          \\
slice\_width                   & 128 ms       & ---          \\
mx\_width                      & 64 ms        & ---          \\
mx\_stride                     & 60 ms        & ---          \\
std\_width                     & 64 ms        & ---          \\
std\_stride                    & 60 ms        & ---          \\
\hline
\end{longtable}
}

\paragraph{Quality}
Both STS and CNN versions observed to produce visually correct results while inferring on a test split -- a separate interval not included neither in train nor in validation split. Numerically the quality can be quantified with landmark error progression. There is the error plot of the STS version trained and evaluated on a EMG2Pose session:

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{error_progression.png}
    \caption{A Inference Error Progression.}
    \label{fig:error_progression}
\end{figure}

Overall the validation metrics achieved by both models across training individually on all sessions are presented in the table:

{\small
\begin{longtable}{l|c|l}
\captionsetup{justification=centering}
\caption[Comparison of models and EMG input]{Validation error ($\text{mean} \pm \text{std}$) across models and EMG input on different dataset types.} \label{tab:model_emg_comparison} \\
\hline
Setting & Val Error ($\mu \pm \sigma$), mm & Comment \\
\hline
\endfirsthead
\multicolumn{3}{@{}l}{\tablename\ \thetable{} -- continued from previous page} \\
\hline
Setting & Val Error ($\mu \pm \sigma$), mm & Comment \\
\hline
\endhead
\textbf{STS (ours)}           & 22.07 ± 4.15 & -- \\
\textbf{CNN (ours)}           & 20.88 ± 3.45 & -- \\
\textbf{CNN noEMG (ours)}     & 22.10 ± 4.50 & insignificantly worse without EMG \\
\hline
\textbf{STS (emg2pose)}       & 17.63 ± 7.28 & -- \\
\textbf{CNN (emg2pose)}       & 16.89 ± 5.94 & -- \\
\textbf{CNN noEMG (emg2pose)} & 19.31 ± 8.10 & very little degradation without EMG \\
\hline
\textbf{STS (all)}            & 19.37 ± 6.52 & -- \\
\textbf{CNN (all)}            & 19.25 ± 4.94 & matching quality of STS \\
\textbf{CNN noEMG (all)}      & 20.55 ± 6.80 & error grows insignificantly without EMG \\
\hline
\end{longtable}
}

From the information above it's clearly seen that STS and CNN featurizers are nearly the same in mean quality. However STS provides more diverse results and we need to comment that for a better quality dataset it fits better than CNN, but for a slight loss in quality it reacts much seriously. -- Generally it means that better models can be constructed with STS if a better EMG signal provided.

Additionally, for a record on our hardware of a repeatable hand motion, we achieve precisely \textbf{17 mm} mean landmark error, indicating that the landmark errors for both hand kinematic models are fair to compare. -- Overall concluding that having random motion in the dataset indeed helps the model to attend to EMG more rather than spending resources trying to fake the performance by leveraging predictable hand motion of the training split.

At the same time, we observe \textbf{CNN noEMG (ours)} not to meet the expected degradation level. Since degradation consist of two things: hand pose predictability and the EMG signal quality itself, it appears that the second option is hit -- the EMG acquisition hardware we use does not meet good signal quality.

\paragraph{Model Size}
STS appears to be more expensive in terms of parameters while having the same quality. In case of the benchmarked models, STS weights \textbf{3.2 MB} with \textbf{840 K} parameters, while CNN version is just acquiring \textbf{1 MB} with \textbf{250 K} parameters.

\paragraph{Training and Inference Time}
Because of the size of the models, while benchmarking the Phase II, mean time to train a STS model is observed to be about \textbf{1.5 hours}. On the other hand, mean time to train a CNN version is measured to be about \textbf{1 hour}. -- This indicates that STS is faster to execute under the same number of parameters, but the fact that it required more parameters to achieve the same quality outbalanced the training time.

Similar situation appears at the inference time. Where CNN model is able to perform \textbf{1200 predictions per second} on a \textbf{AMD Ryzen 7 7735HS} CPU. While for a bigger STS model prediction rate is about \textbf{900 predictions per second} on the same hardware.